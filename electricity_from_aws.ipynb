{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "occupational-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.4.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.19.63)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (0.6.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "original-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "saved-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is base class used for prediction. It can be at top and not dependent on anyone except imports\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, \n",
    "                         #serializer=JSONSerializer(),\n",
    "                         serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "                         **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        \n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.date_range(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-cowboy",
   "metadata": {},
   "source": [
    "As I have already loaded csv file to s3, I will use it to create timeseries list which will be required to pass for prediction.\n",
    "This is step not required to do now, but will be eventually needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accurate-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 's3://sagemaker-us-east-1-686433372380/sagemaker/electricity/electric.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charming-flight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MT_001</th>\n",
       "      <th>MT_002</th>\n",
       "      <th>MT_003</th>\n",
       "      <th>MT_004</th>\n",
       "      <th>MT_005</th>\n",
       "      <th>MT_006</th>\n",
       "      <th>MT_007</th>\n",
       "      <th>MT_008</th>\n",
       "      <th>MT_009</th>\n",
       "      <th>MT_010</th>\n",
       "      <th>...</th>\n",
       "      <th>MT_091</th>\n",
       "      <th>MT_092</th>\n",
       "      <th>MT_093</th>\n",
       "      <th>MT_094</th>\n",
       "      <th>MT_095</th>\n",
       "      <th>MT_096</th>\n",
       "      <th>MT_097</th>\n",
       "      <th>MT_098</th>\n",
       "      <th>MT_099</th>\n",
       "      <th>MT_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:15:00</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>23.470839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.406504</td>\n",
       "      <td>73.170732</td>\n",
       "      <td>288.690476</td>\n",
       "      <td>6.218202</td>\n",
       "      <td>235.690236</td>\n",
       "      <td>47.202797</td>\n",
       "      <td>67.741935</td>\n",
       "      <td>...</td>\n",
       "      <td>67.378252</td>\n",
       "      <td>32.710280</td>\n",
       "      <td>1.015228</td>\n",
       "      <td>7.074280</td>\n",
       "      <td>3.143007</td>\n",
       "      <td>6.376812</td>\n",
       "      <td>157.563025</td>\n",
       "      <td>252.059308</td>\n",
       "      <td>402.34375</td>\n",
       "      <td>10.652463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:30:00</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>23.470839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.373984</td>\n",
       "      <td>78.048780</td>\n",
       "      <td>267.857143</td>\n",
       "      <td>6.783493</td>\n",
       "      <td>252.525253</td>\n",
       "      <td>52.447552</td>\n",
       "      <td>67.741935</td>\n",
       "      <td>...</td>\n",
       "      <td>66.044029</td>\n",
       "      <td>33.229491</td>\n",
       "      <td>1.015228</td>\n",
       "      <td>10.106114</td>\n",
       "      <td>3.143007</td>\n",
       "      <td>6.956522</td>\n",
       "      <td>170.168067</td>\n",
       "      <td>245.469522</td>\n",
       "      <td>394.53125</td>\n",
       "      <td>11.318242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:45:00</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>24.182077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.308943</td>\n",
       "      <td>74.390244</td>\n",
       "      <td>255.952381</td>\n",
       "      <td>6.218202</td>\n",
       "      <td>249.158249</td>\n",
       "      <td>50.699301</td>\n",
       "      <td>63.440860</td>\n",
       "      <td>...</td>\n",
       "      <td>70.713809</td>\n",
       "      <td>33.229491</td>\n",
       "      <td>1.522843</td>\n",
       "      <td>7.579586</td>\n",
       "      <td>3.143007</td>\n",
       "      <td>5.797101</td>\n",
       "      <td>174.369748</td>\n",
       "      <td>200.988468</td>\n",
       "      <td>257.81250</td>\n",
       "      <td>10.652463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 01:00:00</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>23.470839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.178862</td>\n",
       "      <td>74.390244</td>\n",
       "      <td>252.976190</td>\n",
       "      <td>6.218202</td>\n",
       "      <td>249.158249</td>\n",
       "      <td>52.447552</td>\n",
       "      <td>72.043011</td>\n",
       "      <td>...</td>\n",
       "      <td>72.715143</td>\n",
       "      <td>33.229491</td>\n",
       "      <td>1.522843</td>\n",
       "      <td>6.568974</td>\n",
       "      <td>3.143007</td>\n",
       "      <td>5.797101</td>\n",
       "      <td>172.268908</td>\n",
       "      <td>191.103789</td>\n",
       "      <td>253.90625</td>\n",
       "      <td>9.986684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 01:15:00</th>\n",
       "      <td>3.807107</td>\n",
       "      <td>22.759602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.439024</td>\n",
       "      <td>70.731707</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>6.783493</td>\n",
       "      <td>252.525253</td>\n",
       "      <td>52.447552</td>\n",
       "      <td>69.892473</td>\n",
       "      <td>...</td>\n",
       "      <td>74.716478</td>\n",
       "      <td>33.229491</td>\n",
       "      <td>1.015228</td>\n",
       "      <td>8.590197</td>\n",
       "      <td>3.143007</td>\n",
       "      <td>6.956522</td>\n",
       "      <td>165.966387</td>\n",
       "      <td>210.873147</td>\n",
       "      <td>261.71875</td>\n",
       "      <td>10.652463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       MT_001     MT_002  MT_003      MT_004     MT_005  \\\n",
       "2014-01-01 00:15:00  2.538071  23.470839     0.0  150.406504  73.170732   \n",
       "2014-01-01 00:30:00  2.538071  23.470839     0.0  148.373984  78.048780   \n",
       "2014-01-01 00:45:00  2.538071  24.182077     0.0  144.308943  74.390244   \n",
       "2014-01-01 01:00:00  2.538071  23.470839     0.0  136.178862  74.390244   \n",
       "2014-01-01 01:15:00  3.807107  22.759602     0.0  152.439024  70.731707   \n",
       "\n",
       "                         MT_006    MT_007      MT_008     MT_009     MT_010  \\\n",
       "2014-01-01 00:15:00  288.690476  6.218202  235.690236  47.202797  67.741935   \n",
       "2014-01-01 00:30:00  267.857143  6.783493  252.525253  52.447552  67.741935   \n",
       "2014-01-01 00:45:00  255.952381  6.218202  249.158249  50.699301  63.440860   \n",
       "2014-01-01 01:00:00  252.976190  6.218202  249.158249  52.447552  72.043011   \n",
       "2014-01-01 01:15:00  250.000000  6.783493  252.525253  52.447552  69.892473   \n",
       "\n",
       "                     ...     MT_091     MT_092    MT_093     MT_094    MT_095  \\\n",
       "2014-01-01 00:15:00  ...  67.378252  32.710280  1.015228   7.074280  3.143007   \n",
       "2014-01-01 00:30:00  ...  66.044029  33.229491  1.015228  10.106114  3.143007   \n",
       "2014-01-01 00:45:00  ...  70.713809  33.229491  1.522843   7.579586  3.143007   \n",
       "2014-01-01 01:00:00  ...  72.715143  33.229491  1.522843   6.568974  3.143007   \n",
       "2014-01-01 01:15:00  ...  74.716478  33.229491  1.015228   8.590197  3.143007   \n",
       "\n",
       "                       MT_096      MT_097      MT_098     MT_099     MT_100  \n",
       "2014-01-01 00:15:00  6.376812  157.563025  252.059308  402.34375  10.652463  \n",
       "2014-01-01 00:30:00  6.956522  170.168067  245.469522  394.53125  11.318242  \n",
       "2014-01-01 00:45:00  5.797101  174.369748  200.988468  257.81250  10.652463  \n",
       "2014-01-01 01:00:00  5.797101  172.268908  191.103789  253.90625   9.986684  \n",
       "2014-01-01 01:15:00  6.956522  165.966387  210.873147  261.71875  10.652463  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It worked well and csv is loaded as table in pandas\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ambient-shark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MT_001</th>\n",
       "      <th>MT_002</th>\n",
       "      <th>MT_003</th>\n",
       "      <th>MT_004</th>\n",
       "      <th>MT_005</th>\n",
       "      <th>MT_006</th>\n",
       "      <th>MT_007</th>\n",
       "      <th>MT_008</th>\n",
       "      <th>MT_009</th>\n",
       "      <th>MT_010</th>\n",
       "      <th>...</th>\n",
       "      <th>MT_091</th>\n",
       "      <th>MT_092</th>\n",
       "      <th>MT_093</th>\n",
       "      <th>MT_094</th>\n",
       "      <th>MT_095</th>\n",
       "      <th>MT_096</th>\n",
       "      <th>MT_097</th>\n",
       "      <th>MT_098</th>\n",
       "      <th>MT_099</th>\n",
       "      <th>MT_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:00:00</th>\n",
       "      <td>2.379442</td>\n",
       "      <td>20.536984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.302846</td>\n",
       "      <td>63.719512</td>\n",
       "      <td>218.377976</td>\n",
       "      <td>5.370266</td>\n",
       "      <td>217.171717</td>\n",
       "      <td>44.798951</td>\n",
       "      <td>59.543011</td>\n",
       "      <td>...</td>\n",
       "      <td>61.957972</td>\n",
       "      <td>29.075805</td>\n",
       "      <td>1.078680</td>\n",
       "      <td>7.263770</td>\n",
       "      <td>2.750131</td>\n",
       "      <td>5.797101</td>\n",
       "      <td>145.220588</td>\n",
       "      <td>190.691928</td>\n",
       "      <td>265.136719</td>\n",
       "      <td>9.404128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 02:00:00</th>\n",
       "      <td>2.855330</td>\n",
       "      <td>23.204125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.654472</td>\n",
       "      <td>64.786585</td>\n",
       "      <td>196.056548</td>\n",
       "      <td>5.723573</td>\n",
       "      <td>229.797980</td>\n",
       "      <td>49.825175</td>\n",
       "      <td>59.543011</td>\n",
       "      <td>...</td>\n",
       "      <td>72.548366</td>\n",
       "      <td>32.450675</td>\n",
       "      <td>1.522843</td>\n",
       "      <td>8.653360</td>\n",
       "      <td>3.470403</td>\n",
       "      <td>6.956522</td>\n",
       "      <td>163.077731</td>\n",
       "      <td>184.925865</td>\n",
       "      <td>252.441406</td>\n",
       "      <td>10.236352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 04:00:00</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>21.870555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.231707</td>\n",
       "      <td>60.213415</td>\n",
       "      <td>149.925595</td>\n",
       "      <td>4.663652</td>\n",
       "      <td>183.080808</td>\n",
       "      <td>41.083916</td>\n",
       "      <td>52.016129</td>\n",
       "      <td>...</td>\n",
       "      <td>75.633756</td>\n",
       "      <td>32.320872</td>\n",
       "      <td>1.205584</td>\n",
       "      <td>8.716523</td>\n",
       "      <td>3.273965</td>\n",
       "      <td>7.028986</td>\n",
       "      <td>163.340336</td>\n",
       "      <td>183.690280</td>\n",
       "      <td>187.011719</td>\n",
       "      <td>10.985353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 06:00:00</th>\n",
       "      <td>3.013959</td>\n",
       "      <td>21.781650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.495935</td>\n",
       "      <td>58.384146</td>\n",
       "      <td>142.485119</td>\n",
       "      <td>4.592990</td>\n",
       "      <td>154.882155</td>\n",
       "      <td>37.587413</td>\n",
       "      <td>49.865591</td>\n",
       "      <td>...</td>\n",
       "      <td>71.214143</td>\n",
       "      <td>44.911734</td>\n",
       "      <td>1.522843</td>\n",
       "      <td>7.769075</td>\n",
       "      <td>20.560503</td>\n",
       "      <td>7.246377</td>\n",
       "      <td>155.462185</td>\n",
       "      <td>177.924217</td>\n",
       "      <td>256.835938</td>\n",
       "      <td>10.069907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 08:00:00</th>\n",
       "      <td>0.793147</td>\n",
       "      <td>24.004267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.792683</td>\n",
       "      <td>46.951220</td>\n",
       "      <td>135.416667</td>\n",
       "      <td>4.098361</td>\n",
       "      <td>119.107744</td>\n",
       "      <td>55.069930</td>\n",
       "      <td>38.306452</td>\n",
       "      <td>...</td>\n",
       "      <td>67.878586</td>\n",
       "      <td>51.336968</td>\n",
       "      <td>1.269036</td>\n",
       "      <td>6.442648</td>\n",
       "      <td>15.976951</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>170.430672</td>\n",
       "      <td>207.166392</td>\n",
       "      <td>332.031250</td>\n",
       "      <td>10.486019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       MT_001     MT_002  MT_003      MT_004     MT_005  \\\n",
       "2014-01-01 00:00:00  2.379442  20.536984     0.0  128.302846  63.719512   \n",
       "2014-01-01 02:00:00  2.855330  23.204125     0.0  134.654472  64.786585   \n",
       "2014-01-01 04:00:00  2.538071  21.870555     0.0  108.231707  60.213415   \n",
       "2014-01-01 06:00:00  3.013959  21.781650     0.0   93.495935  58.384146   \n",
       "2014-01-01 08:00:00  0.793147  24.004267     0.0   80.792683  46.951220   \n",
       "\n",
       "                         MT_006    MT_007      MT_008     MT_009     MT_010  \\\n",
       "2014-01-01 00:00:00  218.377976  5.370266  217.171717  44.798951  59.543011   \n",
       "2014-01-01 02:00:00  196.056548  5.723573  229.797980  49.825175  59.543011   \n",
       "2014-01-01 04:00:00  149.925595  4.663652  183.080808  41.083916  52.016129   \n",
       "2014-01-01 06:00:00  142.485119  4.592990  154.882155  37.587413  49.865591   \n",
       "2014-01-01 08:00:00  135.416667  4.098361  119.107744  55.069930  38.306452   \n",
       "\n",
       "                     ...     MT_091     MT_092    MT_093    MT_094     MT_095  \\\n",
       "2014-01-01 00:00:00  ...  61.957972  29.075805  1.078680  7.263770   2.750131   \n",
       "2014-01-01 02:00:00  ...  72.548366  32.450675  1.522843  8.653360   3.470403   \n",
       "2014-01-01 04:00:00  ...  75.633756  32.320872  1.205584  8.716523   3.273965   \n",
       "2014-01-01 06:00:00  ...  71.214143  44.911734  1.522843  7.769075  20.560503   \n",
       "2014-01-01 08:00:00  ...  67.878586  51.336968  1.269036  6.442648  15.976951   \n",
       "\n",
       "                       MT_096      MT_097      MT_098      MT_099     MT_100  \n",
       "2014-01-01 00:00:00  5.797101  145.220588  190.691928  265.136719   9.404128  \n",
       "2014-01-01 02:00:00  6.956522  163.077731  184.925865  252.441406  10.236352  \n",
       "2014-01-01 04:00:00  7.028986  163.340336  183.690280  187.011719  10.985353  \n",
       "2014-01-01 06:00:00  7.246377  155.462185  177.924217  256.835938  10.069907  \n",
       "2014-01-01 08:00:00  6.666667  170.430672  207.166392  332.031250  10.486019  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now all steps to create timeseries list of lists to pass for predition at end of the process.\n",
    "data_kw = df.resample('2H').sum() / 8\n",
    "data_kw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "delayed-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = []\n",
    "num_timeseries = df.shape[1]\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_kw.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "devoted-austria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014-01-01 00:00:00    2.379442\n",
       "2014-01-01 02:00:00    2.855330\n",
       "2014-01-01 04:00:00    2.538071\n",
       "2014-01-01 06:00:00    3.013959\n",
       "2014-01-01 08:00:00    0.793147\n",
       "2014-01-01 10:00:00    0.475888\n",
       "2014-01-01 12:00:00    0.475888\n",
       "2014-01-01 14:00:00    0.475888\n",
       "2014-01-01 16:00:00    2.220812\n",
       "2014-01-01 18:00:00    2.855330\n",
       "Freq: 2H, Name: MT_001, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(timeseries[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "danish-viewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "4381\n"
     ]
    }
   ],
   "source": [
    "print(len(timeseries))\n",
    "print(len(timeseries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fixed-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries is now ready whenever I need it.\n",
    "# Now I will use train.json and test.json which are stored in s3\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_bucket = 'sagemaker-us-east-1-686433372380'\n",
    "s3_prefix = '/sagemaker/electricity'\n",
    "s3_data_path = 's3://{}{}'.format(s3_bucket, s3_prefix)\n",
    "s3_output_path = 's3://{}{}/output'.format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mechanical-origin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-686433372380/sagemaker/electricity'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "brazilian-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '2H'\n",
    "prediction_length = 3 * 12\n",
    "context_length = 3 * 12\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "#image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "image_name = sagemaker.image_uris.retrieve(region = region, framework = 'forecasting-deepar' ) \n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "israeli-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "recorded-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "senior-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "scheduled-airline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-06 17:45:35 Starting - Starting the training job...\n",
      "2021-02-06 17:45:38 Starting - Launching requested ML instancesProfilerReport-1612633535: InProgress\n",
      ".........\n",
      "2021-02-06 17:47:25 Starting - Preparing the instances for training...\n",
      "2021-02-06 17:48:06 Downloading - Downloading input data...\n",
      "2021-02-06 17:48:26 Training - Downloading the training image.......\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'36', u'epochs': u'400', u'time_freq': u'2H', u'context_length': u'36', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'36', u'time_freq': u'2H', u'context_length': u'36', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Training set statistics:\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Real time series\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] number of time series: 100\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] number of observations: 429688\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] mean target length: 4296\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] min/mean/max target: 0.0/127.465642918/3357.421875\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] mean abs(target): 127.465642918\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] contains missing values: no\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Small number of time series. Doing 7 passes over dataset with prob 0.914285714286 per epoch.\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Test set statistics:\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Real time series\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] number of time series: 100\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] number of observations: 438088\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] mean target length: 4380\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] min/mean/max target: 0.0/127.684221935/3357.421875\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] mean abs(target): 127.684221935\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] contains missing values: no\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] nvidia-smi took: 0.0252339839935 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 214.7970199584961, \"sum\": 214.7970199584961, \"min\": 214.7970199584961}}, \"EndTime\": 1612633784.577533, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633784.361802}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:44 INFO 139692952823616] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 500.9639263153076, \"sum\": 500.9639263153076, \"min\": 500.9639263153076}}, \"EndTime\": 1612633784.862895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633784.577617}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:45 INFO 139692952823616] Epoch[0] Batch[0] avg_epoch_loss=5.214897\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:45 INFO 139692952823616] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=5.21489667892\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:45 INFO 139692952823616] Epoch[0] Batch[5] avg_epoch_loss=4.953066\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:45 INFO 139692952823616] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=4.9530655543\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:45 INFO 139692952823616] Epoch[0] Batch [5]#011Speed: 716.85 samples/sec#011loss=4.953066\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] Epoch[0] Batch[10] avg_epoch_loss=4.943779\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=4.9326341629\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] Epoch[0] Batch [10]#011Speed: 546.66 samples/sec#011loss=4.932634\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 1704.725980758667, \"sum\": 1704.725980758667, \"min\": 1704.725980758667}}, \"EndTime\": 1612633786.567804, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633784.862959}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=382.434977615 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] #quality_metric: host=algo-1, epoch=0, train loss <loss>=4.94377855821\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:46 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_59dac877-da50-42b2-9fb5-e167fcc33a51-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.329010009765625, \"sum\": 33.329010009765625, \"min\": 33.329010009765625}}, \"EndTime\": 1612633786.601871, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633786.5679}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:47 INFO 139692952823616] Epoch[1] Batch[0] avg_epoch_loss=4.756038\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:47 INFO 139692952823616] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=4.75603818893\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:47 INFO 139692952823616] Epoch[1] Batch[5] avg_epoch_loss=4.558354\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:47 INFO 139692952823616] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=4.5583542188\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:47 INFO 139692952823616] Epoch[1] Batch [5]#011Speed: 606.93 samples/sec#011loss=4.558354\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] Epoch[1] Batch[10] avg_epoch_loss=4.530653\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=4.49741153717\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] Epoch[1] Batch [10]#011Speed: 576.47 samples/sec#011loss=4.497412\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1675.7311820983887, \"sum\": 1675.7311820983887, \"min\": 1675.7311820983887}}, \"EndTime\": 1612633788.277772, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633786.601977}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=403.377640695 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] #quality_metric: host=algo-1, epoch=1, train loss <loss>=4.53065299988\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_38ead749-f2b0-4f36-bda4-388e71c64ae2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.66585350036621, \"sum\": 27.66585350036621, \"min\": 27.66585350036621}}, \"EndTime\": 1612633788.306145, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633788.277854}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] Epoch[2] Batch[0] avg_epoch_loss=4.295990\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:48 INFO 139692952823616] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=4.2959895134\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] Epoch[2] Batch[5] avg_epoch_loss=4.289661\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=4.28966116905\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] Epoch[2] Batch [5]#011Speed: 754.58 samples/sec#011loss=4.289661\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1368.7951564788818, \"sum\": 1368.7951564788818, \"min\": 1368.7951564788818}}, \"EndTime\": 1612633789.675067, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633788.306213}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=466.06454031 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] #quality_metric: host=algo-1, epoch=2, train loss <loss>=4.2898768425\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:49 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_9f9aeec9-351b-4542-adb1-235bffb70067-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.40692138671875, \"sum\": 37.40692138671875, \"min\": 37.40692138671875}}, \"EndTime\": 1612633789.713043, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633789.675145}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:50 INFO 139692952823616] Epoch[3] Batch[0] avg_epoch_loss=4.450858\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:50 INFO 139692952823616] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=4.45085811615\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:50 INFO 139692952823616] Epoch[3] Batch[5] avg_epoch_loss=4.299722\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:50 INFO 139692952823616] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=4.29972203573\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:50 INFO 139692952823616] Epoch[3] Batch [5]#011Speed: 772.30 samples/sec#011loss=4.299722\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:51 INFO 139692952823616] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1349.7328758239746, \"sum\": 1349.7328758239746, \"min\": 1349.7328758239746}}, \"EndTime\": 1612633791.062898, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633789.713104}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:51 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=447.455801422 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:51 INFO 139692952823616] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:51 INFO 139692952823616] #quality_metric: host=algo-1, epoch=3, train loss <loss>=4.27013218403\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:51 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:51 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_0bdb8c58-2bf8-4467-a315-8bba810f9fde-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.675058364868164, \"sum\": 25.675058364868164, \"min\": 25.675058364868164}}, \"EndTime\": 1612633791.089223, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633791.062981}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:51 INFO 139692952823616] Epoch[4] Batch[0] avg_epoch_loss=4.112797\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:51 INFO 139692952823616] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.11279678345\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] Epoch[4] Batch[5] avg_epoch_loss=4.082498\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=4.08249827226\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] Epoch[4] Batch [5]#011Speed: 753.51 samples/sec#011loss=4.082498\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] processed a total of 558 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1335.9251022338867, \"sum\": 1335.9251022338867, \"min\": 1335.9251022338867}}, \"EndTime\": 1612633792.425295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633791.089304}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=417.649998474 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] #quality_metric: host=algo-1, epoch=4, train loss <loss>=4.07080464893\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:52 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_1e7afc4e-6b30-474f-878e-b856323d3e23-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.28615760803223, \"sum\": 42.28615760803223, \"min\": 42.28615760803223}}, \"EndTime\": 1612633792.468201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633792.425378}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] Epoch[5] Batch[0] avg_epoch_loss=3.941476\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.94147586823\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] Epoch[5] Batch[5] avg_epoch_loss=3.897504\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=3.89750381311\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] Epoch[5] Batch [5]#011Speed: 753.10 samples/sec#011loss=3.897504\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] Epoch[5] Batch[10] avg_epoch_loss=3.993772\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=4.10929327011\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] Epoch[5] Batch [10]#011Speed: 635.41 samples/sec#011loss=4.109293\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1481.4479351043701, \"sum\": 1481.4479351043701, \"min\": 1481.4479351043701}}, \"EndTime\": 1612633793.949789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633792.468277}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=432.652071824 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] #quality_metric: host=algo-1, epoch=5, train loss <loss>=3.99377174811\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:53 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_33b465df-461d-48a6-992d-2345bd15fdc1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.587995529174805, \"sum\": 42.587995529174805, \"min\": 42.587995529174805}}, \"EndTime\": 1612633793.992977, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633793.949864}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:54 INFO 139692952823616] Epoch[6] Batch[0] avg_epoch_loss=3.698744\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:54 INFO 139692952823616] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.69874429703\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:54 INFO 139692952823616] Epoch[6] Batch[5] avg_epoch_loss=3.769516\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:54 INFO 139692952823616] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.76951559385\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:54 INFO 139692952823616] Epoch[6] Batch [5]#011Speed: 752.06 samples/sec#011loss=3.769516\u001b[0m\n",
      "\n",
      "2021-02-06 17:50:07 Training - Training image download completed. Training in progress.\u001b[34m[02/06/2021 17:49:55 INFO 139692952823616] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1347.91898727417, \"sum\": 1347.91898727417, \"min\": 1347.91898727417}}, \"EndTime\": 1612633795.341031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633793.993052}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:55 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=449.547882653 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:55 INFO 139692952823616] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:55 INFO 139692952823616] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.70418357849\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:55 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:55 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_42ee2787-dc56-42ee-8cc1-325d789475db-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.27706527709961, \"sum\": 26.27706527709961, \"min\": 26.27706527709961}}, \"EndTime\": 1612633795.367912, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633795.341098}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:55 INFO 139692952823616] Epoch[7] Batch[0] avg_epoch_loss=3.717330\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:55 INFO 139692952823616] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.71733045578\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] Epoch[7] Batch[5] avg_epoch_loss=3.741146\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=3.7411463658\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] Epoch[7] Batch [5]#011Speed: 750.32 samples/sec#011loss=3.741146\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] Epoch[7] Batch[10] avg_epoch_loss=3.812566\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=3.89827046394\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] Epoch[7] Batch [10]#011Speed: 630.21 samples/sec#011loss=3.898270\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1473.6480712890625, \"sum\": 1473.6480712890625, \"min\": 1473.6480712890625}}, \"EndTime\": 1612633796.841674, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633795.367973}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=442.405780699 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.81256641041\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:56 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:57 INFO 139692952823616] Epoch[8] Batch[0] avg_epoch_loss=3.759920\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:57 INFO 139692952823616] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=3.7599196434\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:57 INFO 139692952823616] Epoch[8] Batch[5] avg_epoch_loss=3.671325\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:57 INFO 139692952823616] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=3.67132544518\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:57 INFO 139692952823616] Epoch[8] Batch [5]#011Speed: 751.66 samples/sec#011loss=3.671325\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:58 INFO 139692952823616] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1407.4270725250244, \"sum\": 1407.4270725250244, \"min\": 1407.4270725250244}}, \"EndTime\": 1612633798.24968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633796.841751}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:58 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=434.089775111 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:58 INFO 139692952823616] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:58 INFO 139692952823616] #quality_metric: host=algo-1, epoch=8, train loss <loss>=3.73553709984\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:58 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:58 INFO 139692952823616] Epoch[9] Batch[0] avg_epoch_loss=3.918952\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:58 INFO 139692952823616] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.91895151138\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:59 INFO 139692952823616] Epoch[9] Batch[5] avg_epoch_loss=3.711845\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:59 INFO 139692952823616] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.71184500058\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:59 INFO 139692952823616] Epoch[9] Batch [5]#011Speed: 753.00 samples/sec#011loss=3.711845\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:59 INFO 139692952823616] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1360.903024673462, \"sum\": 1360.903024673462, \"min\": 1360.903024673462}}, \"EndTime\": 1612633799.611138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633798.249758}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:59 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=443.049105341 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:59 INFO 139692952823616] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:59 INFO 139692952823616] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.76274654865\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:49:59 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:00 INFO 139692952823616] Epoch[10] Batch[0] avg_epoch_loss=3.820946\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:00 INFO 139692952823616] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.82094550133\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:00 INFO 139692952823616] Epoch[10] Batch[5] avg_epoch_loss=3.690010\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:00 INFO 139692952823616] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=3.69000959396\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:00 INFO 139692952823616] Epoch[10] Batch [5]#011Speed: 723.03 samples/sec#011loss=3.690010\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:01 INFO 139692952823616] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1390.2289867401123, \"sum\": 1390.2289867401123, \"min\": 1390.2289867401123}}, \"EndTime\": 1612633801.001948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633799.611218}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:01 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=427.952920582 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:01 INFO 139692952823616] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:01 INFO 139692952823616] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.70092823505\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:01 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:01 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_566df867-3140-4777-a686-af1a284d5fd3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.343915939331055, \"sum\": 28.343915939331055, \"min\": 28.343915939331055}}, \"EndTime\": 1612633801.030959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633801.002023}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:01 INFO 139692952823616] Epoch[11] Batch[0] avg_epoch_loss=3.905375\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:01 INFO 139692952823616] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=3.90537548065\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] Epoch[11] Batch[5] avg_epoch_loss=3.655730\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=3.65572953224\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] Epoch[11] Batch [5]#011Speed: 667.04 samples/sec#011loss=3.655730\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1524.3961811065674, \"sum\": 1524.3961811065674, \"min\": 1524.3961811065674}}, \"EndTime\": 1612633802.555483, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633801.03103}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=402.752321418 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] #quality_metric: host=algo-1, epoch=11, train loss <loss>=3.60845491886\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:02 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_7d1b76cc-dd1b-4009-b523-8a6e9dfa83cc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.71615028381348, \"sum\": 37.71615028381348, \"min\": 37.71615028381348}}, \"EndTime\": 1612633802.593776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633802.555561}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] Epoch[12] Batch[0] avg_epoch_loss=3.707454\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=3.70745396614\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] Epoch[12] Batch[5] avg_epoch_loss=3.725823\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=3.72582308451\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] Epoch[12] Batch [5]#011Speed: 754.14 samples/sec#011loss=3.725823\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1356.147050857544, \"sum\": 1356.147050857544, \"min\": 1356.147050857544}}, \"EndTime\": 1612633803.95006, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633802.593857}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=436.48678469 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] #quality_metric: host=algo-1, epoch=12, train loss <loss>=3.66903054714\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:03 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:04 INFO 139692952823616] Epoch[13] Batch[0] avg_epoch_loss=3.667694\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:04 INFO 139692952823616] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.66769433022\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:04 INFO 139692952823616] Epoch[13] Batch[5] avg_epoch_loss=3.489845\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:04 INFO 139692952823616] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=3.48984499772\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:04 INFO 139692952823616] Epoch[13] Batch [5]#011Speed: 713.76 samples/sec#011loss=3.489845\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:05 INFO 139692952823616] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1482.5770854949951, \"sum\": 1482.5770854949951, \"min\": 1482.5770854949951}}, \"EndTime\": 1612633805.433251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633803.950162}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:05 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=410.067176977 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:05 INFO 139692952823616] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:05 INFO 139692952823616] #quality_metric: host=algo-1, epoch=13, train loss <loss>=3.51159961224\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:05 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:05 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_08ddcf48-58bf-4f7b-ad04-5f6492eefe47-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.337934494018555, \"sum\": 25.337934494018555, \"min\": 25.337934494018555}}, \"EndTime\": 1612633805.459238, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633805.433321}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:06 INFO 139692952823616] Epoch[14] Batch[0] avg_epoch_loss=3.659081\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:06 INFO 139692952823616] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.65908098221\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:06 INFO 139692952823616] Epoch[14] Batch[5] avg_epoch_loss=3.553674\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:06 INFO 139692952823616] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=3.55367378394\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:06 INFO 139692952823616] Epoch[14] Batch [5]#011Speed: 622.84 samples/sec#011loss=3.553674\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:07 INFO 139692952823616] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1599.2259979248047, \"sum\": 1599.2259979248047, \"min\": 1599.2259979248047}}, \"EndTime\": 1612633807.058595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633805.459307}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:07 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=400.163347643 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:07 INFO 139692952823616] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:07 INFO 139692952823616] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.55139803886\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:07 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:07 INFO 139692952823616] Epoch[15] Batch[0] avg_epoch_loss=3.311430\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:07 INFO 139692952823616] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.311429739\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] Epoch[15] Batch[5] avg_epoch_loss=3.374242\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=3.37424163024\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] Epoch[15] Batch [5]#011Speed: 634.09 samples/sec#011loss=3.374242\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1570.854902267456, \"sum\": 1570.854902267456, \"min\": 1570.854902267456}}, \"EndTime\": 1612633808.630008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633807.058677}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=384.474652102 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] #quality_metric: host=algo-1, epoch=15, train loss <loss>=3.38470127583\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:08 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_80408141-fca9-4b8f-8518-3479410aba94-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 38.3450984954834, \"sum\": 38.3450984954834, \"min\": 38.3450984954834}}, \"EndTime\": 1612633808.669002, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633808.63009}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:09 INFO 139692952823616] Epoch[16] Batch[0] avg_epoch_loss=3.634449\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:09 INFO 139692952823616] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=3.63444948196\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:09 INFO 139692952823616] Epoch[16] Batch[5] avg_epoch_loss=3.540717\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:09 INFO 139692952823616] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.5407170852\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:09 INFO 139692952823616] Epoch[16] Batch [5]#011Speed: 736.27 samples/sec#011loss=3.540717\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] Epoch[16] Batch[10] avg_epoch_loss=3.490374\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=3.42996268272\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] Epoch[16] Batch [10]#011Speed: 589.85 samples/sec#011loss=3.429963\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1550.1999855041504, \"sum\": 1550.1999855041504, \"min\": 1550.1999855041504}}, \"EndTime\": 1612633810.219346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633808.669079}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=416.683321674 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.49037417499\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] Epoch[17] Batch[0] avg_epoch_loss=3.345203\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:10 INFO 139692952823616] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=3.3452026844\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] Epoch[17] Batch[5] avg_epoch_loss=3.401894\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=3.40189441045\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] Epoch[17] Batch [5]#011Speed: 754.29 samples/sec#011loss=3.401894\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] Epoch[17] Batch[10] avg_epoch_loss=3.168536\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=2.88850538805\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] Epoch[17] Batch [10]#011Speed: 668.35 samples/sec#011loss=2.888505\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1447.0298290252686, \"sum\": 1447.0298290252686, \"min\": 1447.0298290252686}}, \"EndTime\": 1612633811.666909, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633810.219444}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=445.01598602 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] #quality_metric: host=algo-1, epoch=17, train loss <loss>=3.16853576391\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:11 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_6f1e9eda-ed76-4c50-a059-784aaaf7b973-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.3928337097168, \"sum\": 34.3928337097168, \"min\": 34.3928337097168}}, \"EndTime\": 1612633811.701872, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633811.666981}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:12 INFO 139692952823616] Epoch[18] Batch[0] avg_epoch_loss=3.381627\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:12 INFO 139692952823616] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.38162660599\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:12 INFO 139692952823616] Epoch[18] Batch[5] avg_epoch_loss=3.415920\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:12 INFO 139692952823616] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.41592013836\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:12 INFO 139692952823616] Epoch[18] Batch [5]#011Speed: 757.73 samples/sec#011loss=3.415920\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] Epoch[18] Batch[10] avg_epoch_loss=3.438643\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=3.46591134071\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] Epoch[18] Batch [10]#011Speed: 594.42 samples/sec#011loss=3.465911\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1525.3889560699463, \"sum\": 1525.3889560699463, \"min\": 1525.3889560699463}}, \"EndTime\": 1612633813.227376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633811.701938}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=420.189822636 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] #quality_metric: host=algo-1, epoch=18, train loss <loss>=3.43864341216\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] Epoch[19] Batch[0] avg_epoch_loss=3.708950\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:13 INFO 139692952823616] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.70894980431\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:14 INFO 139692952823616] Epoch[19] Batch[5] avg_epoch_loss=3.506387\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:14 INFO 139692952823616] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=3.50638695558\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:14 INFO 139692952823616] Epoch[19] Batch [5]#011Speed: 742.44 samples/sec#011loss=3.506387\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:14 INFO 139692952823616] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1411.133050918579, \"sum\": 1411.133050918579, \"min\": 1411.133050918579}}, \"EndTime\": 1612633814.639021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633813.227454}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:14 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=432.240009839 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:14 INFO 139692952823616] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:14 INFO 139692952823616] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.49250972271\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:14 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:15 INFO 139692952823616] Epoch[20] Batch[0] avg_epoch_loss=3.310974\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:15 INFO 139692952823616] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=3.31097388268\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:15 INFO 139692952823616] Epoch[20] Batch[5] avg_epoch_loss=3.465156\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:15 INFO 139692952823616] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=3.46515591939\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:15 INFO 139692952823616] Epoch[20] Batch [5]#011Speed: 746.65 samples/sec#011loss=3.465156\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] Epoch[20] Batch[10] avg_epoch_loss=3.545791\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=3.64255409241\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] Epoch[20] Batch [10]#011Speed: 625.59 samples/sec#011loss=3.642554\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1526.123046875, \"sum\": 1526.123046875, \"min\": 1526.123046875}}, \"EndTime\": 1612633816.165695, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633814.639103}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=427.194796467 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] #quality_metric: host=algo-1, epoch=20, train loss <loss>=3.54579145258\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] Epoch[21] Batch[0] avg_epoch_loss=3.344280\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:16 INFO 139692952823616] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=3.34427976608\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:17 INFO 139692952823616] Epoch[21] Batch[5] avg_epoch_loss=3.420303\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:17 INFO 139692952823616] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=3.42030255\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:17 INFO 139692952823616] Epoch[21] Batch [5]#011Speed: 733.60 samples/sec#011loss=3.420303\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:17 INFO 139692952823616] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1403.7449359893799, \"sum\": 1403.7449359893799, \"min\": 1403.7449359893799}}, \"EndTime\": 1612633817.569996, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633816.165772}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:17 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=433.801775571 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:17 INFO 139692952823616] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:17 INFO 139692952823616] #quality_metric: host=algo-1, epoch=21, train loss <loss>=3.4697072506\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:17 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:18 INFO 139692952823616] Epoch[22] Batch[0] avg_epoch_loss=3.205305\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:18 INFO 139692952823616] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=3.20530509949\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:18 INFO 139692952823616] Epoch[22] Batch[5] avg_epoch_loss=3.313171\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:18 INFO 139692952823616] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=3.31317055225\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:18 INFO 139692952823616] Epoch[22] Batch [5]#011Speed: 714.21 samples/sec#011loss=3.313171\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] Epoch[22] Batch[10] avg_epoch_loss=3.364534\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=3.4261698246\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] Epoch[22] Batch [10]#011Speed: 653.15 samples/sec#011loss=3.426170\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1502.8529167175293, \"sum\": 1502.8529167175293, \"min\": 1502.8529167175293}}, \"EndTime\": 1612633819.073452, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633817.570079}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=441.127135006 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.36453385787\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] Epoch[23] Batch[0] avg_epoch_loss=3.165282\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:19 INFO 139692952823616] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.16528224945\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] Epoch[23] Batch[5] avg_epoch_loss=3.306481\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=3.3064806064\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] Epoch[23] Batch [5]#011Speed: 740.90 samples/sec#011loss=3.306481\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] Epoch[23] Batch[10] avg_epoch_loss=3.251411\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=3.18532705307\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] Epoch[23] Batch [10]#011Speed: 657.29 samples/sec#011loss=3.185327\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1459.6819877624512, \"sum\": 1459.6819877624512, \"min\": 1459.6819877624512}}, \"EndTime\": 1612633820.53369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633819.07353}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=439.101890692 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] #quality_metric: host=algo-1, epoch=23, train loss <loss>=3.25141080943\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:20 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] Epoch[24] Batch[0] avg_epoch_loss=3.504450\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=3.50444960594\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] Epoch[24] Batch[5] avg_epoch_loss=3.425428\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.42542823156\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] Epoch[24] Batch [5]#011Speed: 759.58 samples/sec#011loss=3.425428\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1422.1198558807373, \"sum\": 1422.1198558807373, \"min\": 1422.1198558807373}}, \"EndTime\": 1612633821.956368, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633820.533768}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=436.634868492 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] #quality_metric: host=algo-1, epoch=24, train loss <loss>=3.45256619453\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:21 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:22 INFO 139692952823616] Epoch[25] Batch[0] avg_epoch_loss=3.493074\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:22 INFO 139692952823616] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=3.49307441711\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:22 INFO 139692952823616] Epoch[25] Batch[5] avg_epoch_loss=3.441888\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:22 INFO 139692952823616] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=3.44188805421\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:22 INFO 139692952823616] Epoch[25] Batch [5]#011Speed: 727.36 samples/sec#011loss=3.441888\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:23 INFO 139692952823616] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1426.0101318359375, \"sum\": 1426.0101318359375, \"min\": 1426.0101318359375}}, \"EndTime\": 1612633823.382955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633821.956451}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:23 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=446.663628693 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:23 INFO 139692952823616] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:23 INFO 139692952823616] #quality_metric: host=algo-1, epoch=25, train loss <loss>=3.45648002625\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:23 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:23 INFO 139692952823616] Epoch[26] Batch[0] avg_epoch_loss=3.176335\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:23 INFO 139692952823616] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=3.17633509636\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] Epoch[26] Batch[5] avg_epoch_loss=3.301951\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=3.30195085208\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] Epoch[26] Batch [5]#011Speed: 753.69 samples/sec#011loss=3.301951\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] Epoch[26] Batch[10] avg_epoch_loss=3.268287\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=3.22788982391\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] Epoch[26] Batch [10]#011Speed: 620.63 samples/sec#011loss=3.227890\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1510.7688903808594, \"sum\": 1510.7688903808594, \"min\": 1510.7688903808594}}, \"EndTime\": 1612633824.894273, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633823.383036}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=430.21054977 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] #quality_metric: host=algo-1, epoch=26, train loss <loss>=3.26828674837\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:24 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:25 INFO 139692952823616] Epoch[27] Batch[0] avg_epoch_loss=3.320038\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:25 INFO 139692952823616] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.32003760338\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:25 INFO 139692952823616] Epoch[27] Batch[5] avg_epoch_loss=3.244151\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:25 INFO 139692952823616] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=3.24415071805\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:25 INFO 139692952823616] Epoch[27] Batch [5]#011Speed: 733.67 samples/sec#011loss=3.244151\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:26 INFO 139692952823616] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1395.7929611206055, \"sum\": 1395.7929611206055, \"min\": 1395.7929611206055}}, \"EndTime\": 1612633826.290604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633824.894354}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:26 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=436.989646941 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:26 INFO 139692952823616] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:26 INFO 139692952823616] #quality_metric: host=algo-1, epoch=27, train loss <loss>=3.31159975529\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:26 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:26 INFO 139692952823616] Epoch[28] Batch[0] avg_epoch_loss=3.408377\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:26 INFO 139692952823616] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=3.40837717056\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:27 INFO 139692952823616] Epoch[28] Batch[5] avg_epoch_loss=3.409742\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:27 INFO 139692952823616] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=3.4097417593\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:27 INFO 139692952823616] Epoch[28] Batch [5]#011Speed: 726.10 samples/sec#011loss=3.409742\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:27 INFO 139692952823616] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1448.606014251709, \"sum\": 1448.606014251709, \"min\": 1448.606014251709}}, \"EndTime\": 1612633827.739778, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633826.290686}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:27 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=439.696979442 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:27 INFO 139692952823616] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:27 INFO 139692952823616] #quality_metric: host=algo-1, epoch=28, train loss <loss>=3.44381661415\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:27 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:28 INFO 139692952823616] Epoch[29] Batch[0] avg_epoch_loss=3.294550\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:28 INFO 139692952823616] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.29455018044\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:28 INFO 139692952823616] Epoch[29] Batch[5] avg_epoch_loss=3.345272\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:28 INFO 139692952823616] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.34527246157\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:28 INFO 139692952823616] Epoch[29] Batch [5]#011Speed: 760.29 samples/sec#011loss=3.345272\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:29 INFO 139692952823616] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1381.0031414031982, \"sum\": 1381.0031414031982, \"min\": 1381.0031414031982}}, \"EndTime\": 1612633829.121331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633827.739859}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:29 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=454.70508851 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:29 INFO 139692952823616] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:29 INFO 139692952823616] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.38806488514\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:29 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:29 INFO 139692952823616] Epoch[30] Batch[0] avg_epoch_loss=3.590497\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:29 INFO 139692952823616] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.59049701691\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:30 INFO 139692952823616] Epoch[30] Batch[5] avg_epoch_loss=3.364448\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:30 INFO 139692952823616] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=3.36444759369\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:30 INFO 139692952823616] Epoch[30] Batch [5]#011Speed: 763.25 samples/sec#011loss=3.364448\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:30 INFO 139692952823616] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1366.4991855621338, \"sum\": 1366.4991855621338, \"min\": 1366.4991855621338}}, \"EndTime\": 1612633830.488404, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633829.121404}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:30 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=464.649990754 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:30 INFO 139692952823616] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:30 INFO 139692952823616] #quality_metric: host=algo-1, epoch=30, train loss <loss>=3.37770411968\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:30 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] Epoch[31] Batch[0] avg_epoch_loss=3.485454\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=3.48545360565\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] Epoch[31] Batch[5] avg_epoch_loss=3.275247\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=3.27524693807\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] Epoch[31] Batch [5]#011Speed: 759.51 samples/sec#011loss=3.275247\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] processed a total of 581 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1354.722023010254, \"sum\": 1354.722023010254, \"min\": 1354.722023010254}}, \"EndTime\": 1612633831.843719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633830.488486}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=428.837364621 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] #quality_metric: host=algo-1, epoch=31, train loss <loss>=3.27587485313\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:31 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:32 INFO 139692952823616] Epoch[32] Batch[0] avg_epoch_loss=3.116643\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:32 INFO 139692952823616] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=3.11664295197\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:32 INFO 139692952823616] Epoch[32] Batch[5] avg_epoch_loss=3.312616\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:32 INFO 139692952823616] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=3.31261630853\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:32 INFO 139692952823616] Epoch[32] Batch [5]#011Speed: 747.65 samples/sec#011loss=3.312616\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:33 INFO 139692952823616] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1419.494867324829, \"sum\": 1419.494867324829, \"min\": 1419.494867324829}}, \"EndTime\": 1612633833.263774, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633831.843787}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:33 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=448.716664447 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:33 INFO 139692952823616] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:33 INFO 139692952823616] #quality_metric: host=algo-1, epoch=32, train loss <loss>=3.28376166821\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:33 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:33 INFO 139692952823616] Epoch[33] Batch[0] avg_epoch_loss=3.247247\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:33 INFO 139692952823616] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=3.24724674225\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:34 INFO 139692952823616] Epoch[33] Batch[5] avg_epoch_loss=3.107451\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:34 INFO 139692952823616] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.10745104154\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:34 INFO 139692952823616] Epoch[33] Batch [5]#011Speed: 767.13 samples/sec#011loss=3.107451\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:34 INFO 139692952823616] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1400.2618789672852, \"sum\": 1400.2618789672852, \"min\": 1400.2618789672852}}, \"EndTime\": 1612633834.664568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633833.263842}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:34 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=445.592006452 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:34 INFO 139692952823616] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:34 INFO 139692952823616] #quality_metric: host=algo-1, epoch=33, train loss <loss>=3.2286318779\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:34 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:35 INFO 139692952823616] Epoch[34] Batch[0] avg_epoch_loss=3.202132\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:35 INFO 139692952823616] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=3.20213198662\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:35 INFO 139692952823616] Epoch[34] Batch[5] avg_epoch_loss=3.449977\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:35 INFO 139692952823616] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=3.44997684161\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:35 INFO 139692952823616] Epoch[34] Batch [5]#011Speed: 751.87 samples/sec#011loss=3.449977\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] Epoch[34] Batch[10] avg_epoch_loss=3.429752\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=3.40548238754\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] Epoch[34] Batch [10]#011Speed: 689.23 samples/sec#011loss=3.405482\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1426.469087600708, \"sum\": 1426.469087600708, \"min\": 1426.469087600708}}, \"EndTime\": 1612633836.091645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633834.664651}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=449.325099394 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.42975208976\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] Epoch[35] Batch[0] avg_epoch_loss=3.175645\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:36 INFO 139692952823616] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=3.17564487457\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] Epoch[35] Batch[5] avg_epoch_loss=3.281905\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=3.28190537294\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] Epoch[35] Batch [5]#011Speed: 766.32 samples/sec#011loss=3.281905\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1360.2268695831299, \"sum\": 1360.2268695831299, \"min\": 1360.2268695831299}}, \"EndTime\": 1612633837.452406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633836.091722}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=452.834358671 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] #quality_metric: host=algo-1, epoch=35, train loss <loss>=3.31512694359\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] Epoch[36] Batch[0] avg_epoch_loss=3.301709\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:37 INFO 139692952823616] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=3.30170917511\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:38 INFO 139692952823616] Epoch[36] Batch[5] avg_epoch_loss=3.237427\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:38 INFO 139692952823616] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=3.23742719491\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:38 INFO 139692952823616] Epoch[36] Batch [5]#011Speed: 693.39 samples/sec#011loss=3.237427\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:38 INFO 139692952823616] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1400.1951217651367, \"sum\": 1400.1951217651367, \"min\": 1400.1951217651367}}, \"EndTime\": 1612633838.853125, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633837.452468}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:38 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=457.040008023 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:38 INFO 139692952823616] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:38 INFO 139692952823616] #quality_metric: host=algo-1, epoch=36, train loss <loss>=3.24490160942\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:38 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:39 INFO 139692952823616] Epoch[37] Batch[0] avg_epoch_loss=3.266284\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:39 INFO 139692952823616] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=3.26628398895\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:39 INFO 139692952823616] Epoch[37] Batch[5] avg_epoch_loss=3.236689\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:39 INFO 139692952823616] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.23668881257\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:39 INFO 139692952823616] Epoch[37] Batch [5]#011Speed: 736.77 samples/sec#011loss=3.236689\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] Epoch[37] Batch[10] avg_epoch_loss=3.212997\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=3.18456635475\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] Epoch[37] Batch [10]#011Speed: 629.46 samples/sec#011loss=3.184566\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1498.4450340270996, \"sum\": 1498.4450340270996, \"min\": 1498.4450340270996}}, \"EndTime\": 1612633840.352126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633838.853207}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=434.417952835 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] #quality_metric: host=algo-1, epoch=37, train loss <loss>=3.21299678629\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] Epoch[38] Batch[0] avg_epoch_loss=3.035756\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:40 INFO 139692952823616] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=3.03575611115\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:41 INFO 139692952823616] Epoch[38] Batch[5] avg_epoch_loss=3.112009\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:41 INFO 139692952823616] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=3.11200877031\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:41 INFO 139692952823616] Epoch[38] Batch [5]#011Speed: 715.17 samples/sec#011loss=3.112009\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:41 INFO 139692952823616] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1395.4710960388184, \"sum\": 1395.4710960388184, \"min\": 1395.4710960388184}}, \"EndTime\": 1612633841.748099, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633840.352203}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:41 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=439.245157488 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:41 INFO 139692952823616] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:41 INFO 139692952823616] #quality_metric: host=algo-1, epoch=38, train loss <loss>=3.32661914825\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:41 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:42 INFO 139692952823616] Epoch[39] Batch[0] avg_epoch_loss=3.219680\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:42 INFO 139692952823616] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=3.21967959404\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:42 INFO 139692952823616] Epoch[39] Batch[5] avg_epoch_loss=3.259130\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:42 INFO 139692952823616] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=3.25912968318\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:42 INFO 139692952823616] Epoch[39] Batch [5]#011Speed: 702.67 samples/sec#011loss=3.259130\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] Epoch[39] Batch[10] avg_epoch_loss=3.300365\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=3.34984641075\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] Epoch[39] Batch [10]#011Speed: 653.92 samples/sec#011loss=3.349846\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1496.3860511779785, \"sum\": 1496.3860511779785, \"min\": 1496.3860511779785}}, \"EndTime\": 1612633843.245101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633841.748172}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=445.037851231 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.30036455935\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] Epoch[40] Batch[0] avg_epoch_loss=3.330974\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:43 INFO 139692952823616] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=3.33097410202\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] Epoch[40] Batch[5] avg_epoch_loss=3.265512\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=3.26551214854\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] Epoch[40] Batch [5]#011Speed: 749.05 samples/sec#011loss=3.265512\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] Epoch[40] Batch[10] avg_epoch_loss=3.024956\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=2.73628915548\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] Epoch[40] Batch [10]#011Speed: 648.20 samples/sec#011loss=2.736289\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1488.8439178466797, \"sum\": 1488.8439178466797, \"min\": 1488.8439178466797}}, \"EndTime\": 1612633844.73445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633843.245179}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=437.2187559 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] #quality_metric: host=algo-1, epoch=40, train loss <loss>=3.0249562426\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:44 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/state_05353b18-74e5-48ab-84d9-ee1f20b9968e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.28008270263672, \"sum\": 37.28008270263672, \"min\": 37.28008270263672}}, \"EndTime\": 1612633844.772303, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633844.734527}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:45 INFO 139692952823616] Epoch[41] Batch[0] avg_epoch_loss=3.442108\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:45 INFO 139692952823616] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=3.44210839272\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:45 INFO 139692952823616] Epoch[41] Batch[5] avg_epoch_loss=3.151753\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:45 INFO 139692952823616] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=3.15175322692\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:45 INFO 139692952823616] Epoch[41] Batch [5]#011Speed: 713.06 samples/sec#011loss=3.151753\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:46 INFO 139692952823616] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1425.990104675293, \"sum\": 1425.990104675293, \"min\": 1425.990104675293}}, \"EndTime\": 1612633846.198441, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633844.772382}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:46 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=433.346067832 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:46 INFO 139692952823616] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:46 INFO 139692952823616] #quality_metric: host=algo-1, epoch=41, train loss <loss>=3.1953463316\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:46 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:46 INFO 139692952823616] Epoch[42] Batch[0] avg_epoch_loss=3.376754\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:46 INFO 139692952823616] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=3.37675428391\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:47 INFO 139692952823616] Epoch[42] Batch[5] avg_epoch_loss=3.338508\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:47 INFO 139692952823616] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=3.33850840727\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:47 INFO 139692952823616] Epoch[42] Batch [5]#011Speed: 751.01 samples/sec#011loss=3.338508\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:47 INFO 139692952823616] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1408.139944076538, \"sum\": 1408.139944076538, \"min\": 1408.139944076538}}, \"EndTime\": 1612633847.607148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633846.198523}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:47 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=441.687023307 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:47 INFO 139692952823616] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:47 INFO 139692952823616] #quality_metric: host=algo-1, epoch=42, train loss <loss>=3.25097303391\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:47 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:48 INFO 139692952823616] Epoch[43] Batch[0] avg_epoch_loss=3.129135\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:48 INFO 139692952823616] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=3.12913489342\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:48 INFO 139692952823616] Epoch[43] Batch[5] avg_epoch_loss=3.064116\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:48 INFO 139692952823616] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=3.06411572297\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:48 INFO 139692952823616] Epoch[43] Batch [5]#011Speed: 711.00 samples/sec#011loss=3.064116\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1407.8559875488281, \"sum\": 1407.8559875488281, \"min\": 1407.8559875488281}}, \"EndTime\": 1612633849.015551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633847.607212}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=425.440618799 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] #quality_metric: host=algo-1, epoch=43, train loss <loss>=3.05185499191\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] Epoch[44] Batch[0] avg_epoch_loss=3.390571\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=3.3905711174\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] Epoch[44] Batch[5] avg_epoch_loss=3.340875\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=3.34087455273\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:49 INFO 139692952823616] Epoch[44] Batch [5]#011Speed: 741.60 samples/sec#011loss=3.340875\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] Epoch[44] Batch[10] avg_epoch_loss=3.263053\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=3.16966762543\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] Epoch[44] Batch [10]#011Speed: 688.88 samples/sec#011loss=3.169668\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1435.412883758545, \"sum\": 1435.412883758545, \"min\": 1435.412883758545}}, \"EndTime\": 1612633850.451495, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633849.015613}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=465.332373591 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] #quality_metric: host=algo-1, epoch=44, train loss <loss>=3.26305322214\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] Epoch[45] Batch[0] avg_epoch_loss=3.540152\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:50 INFO 139692952823616] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=3.54015207291\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:51 INFO 139692952823616] Epoch[45] Batch[5] avg_epoch_loss=3.282473\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:51 INFO 139692952823616] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=3.28247328599\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:51 INFO 139692952823616] Epoch[45] Batch [5]#011Speed: 731.69 samples/sec#011loss=3.282473\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:51 INFO 139692952823616] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1367.7282333374023, \"sum\": 1367.7282333374023, \"min\": 1367.7282333374023}}, \"EndTime\": 1612633851.819811, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633850.451575}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:51 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=447.41745865 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:51 INFO 139692952823616] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:51 INFO 139692952823616] #quality_metric: host=algo-1, epoch=45, train loss <loss>=3.26728060246\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:51 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:52 INFO 139692952823616] Epoch[46] Batch[0] avg_epoch_loss=3.081570\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:52 INFO 139692952823616] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=3.08156991005\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:52 INFO 139692952823616] Epoch[46] Batch[5] avg_epoch_loss=3.302793\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:52 INFO 139692952823616] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=3.30279258887\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:52 INFO 139692952823616] Epoch[46] Batch [5]#011Speed: 715.62 samples/sec#011loss=3.302793\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:53 INFO 139692952823616] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1346.4388847351074, \"sum\": 1346.4388847351074, \"min\": 1346.4388847351074}}, \"EndTime\": 1612633853.166785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633851.819893}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:53 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=427.018783711 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:53 INFO 139692952823616] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:53 INFO 139692952823616] #quality_metric: host=algo-1, epoch=46, train loss <loss>=3.26040328874\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:53 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:53 INFO 139692952823616] Epoch[47] Batch[0] avg_epoch_loss=3.130677\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:53 INFO 139692952823616] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=3.13067698479\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] Epoch[47] Batch[5] avg_epoch_loss=3.198298\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=3.19829813639\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] Epoch[47] Batch [5]#011Speed: 681.54 samples/sec#011loss=3.198298\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] Epoch[47] Batch[10] avg_epoch_loss=3.344401\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=3.51972489357\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] Epoch[47] Batch [10]#011Speed: 675.52 samples/sec#011loss=3.519725\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1492.130994796753, \"sum\": 1492.130994796753, \"min\": 1492.130994796753}}, \"EndTime\": 1612633854.659524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633853.166855}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=440.275044392 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] #quality_metric: host=algo-1, epoch=47, train loss <loss>=3.34440120784\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:54 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:55 INFO 139692952823616] Epoch[48] Batch[0] avg_epoch_loss=3.504117\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:55 INFO 139692952823616] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=3.50411701202\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:55 INFO 139692952823616] Epoch[48] Batch[5] avg_epoch_loss=3.194977\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:55 INFO 139692952823616] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=3.1949767669\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:55 INFO 139692952823616] Epoch[48] Batch [5]#011Speed: 754.77 samples/sec#011loss=3.194977\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1362.3340129852295, \"sum\": 1362.3340129852295, \"min\": 1362.3340129852295}}, \"EndTime\": 1612633856.022474, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633854.659601}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=459.463707724 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] #quality_metric: host=algo-1, epoch=48, train loss <loss>=3.31952371597\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] Epoch[49] Batch[0] avg_epoch_loss=2.990394\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.99039435387\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] Epoch[49] Batch[5] avg_epoch_loss=3.196796\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=3.19679586093\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:56 INFO 139692952823616] Epoch[49] Batch [5]#011Speed: 762.58 samples/sec#011loss=3.196796\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:57 INFO 139692952823616] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1330.7950496673584, \"sum\": 1330.7950496673584, \"min\": 1330.7950496673584}}, \"EndTime\": 1612633857.353817, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633856.022559}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:57 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=468.851814279 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:57 INFO 139692952823616] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:57 INFO 139692952823616] #quality_metric: host=algo-1, epoch=49, train loss <loss>=3.21253628731\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:57 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:57 INFO 139692952823616] Epoch[50] Batch[0] avg_epoch_loss=3.385645\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:57 INFO 139692952823616] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=3.38564538956\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] Epoch[50] Batch[5] avg_epoch_loss=3.263922\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=3.26392189662\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] Epoch[50] Batch [5]#011Speed: 739.21 samples/sec#011loss=3.263922\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] Epoch[50] Batch[10] avg_epoch_loss=3.221862\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=3.17139024734\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] Epoch[50] Batch [10]#011Speed: 641.46 samples/sec#011loss=3.171390\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1494.6539402008057, \"sum\": 1494.6539402008057, \"min\": 1494.6539402008057}}, \"EndTime\": 1612633858.849069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633857.353897}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=453.58299156 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] #quality_metric: host=algo-1, epoch=50, train loss <loss>=3.22186205604\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:58 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:59 INFO 139692952823616] Epoch[51] Batch[0] avg_epoch_loss=3.203705\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:59 INFO 139692952823616] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=3.20370483398\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:59 INFO 139692952823616] Epoch[51] Batch[5] avg_epoch_loss=3.164839\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:59 INFO 139692952823616] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=3.16483859221\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:50:59 INFO 139692952823616] Epoch[51] Batch [5]#011Speed: 760.18 samples/sec#011loss=3.164839\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] Epoch[51] Batch[10] avg_epoch_loss=3.139588\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=3.10928740501\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] Epoch[51] Batch [10]#011Speed: 651.81 samples/sec#011loss=3.109287\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1467.245101928711, \"sum\": 1467.245101928711, \"min\": 1467.245101928711}}, \"EndTime\": 1612633860.316813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633858.849144}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=441.612715752 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] #quality_metric: host=algo-1, epoch=51, train loss <loss>=3.13958805258\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] Epoch[52] Batch[0] avg_epoch_loss=3.121420\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:00 INFO 139692952823616] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=3.1214196682\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] Epoch[52] Batch[5] avg_epoch_loss=3.214579\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=3.21457918485\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] Epoch[52] Batch [5]#011Speed: 762.30 samples/sec#011loss=3.214579\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] Epoch[52] Batch[10] avg_epoch_loss=3.505059\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=3.85363502502\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] Epoch[52] Batch [10]#011Speed: 667.51 samples/sec#011loss=3.853635\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1440.3941631317139, \"sum\": 1440.3941631317139, \"min\": 1440.3941631317139}}, \"EndTime\": 1612633861.757775, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633860.316882}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=451.229366496 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] #quality_metric: host=algo-1, epoch=52, train loss <loss>=3.5050591122\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:01 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:02 INFO 139692952823616] Epoch[53] Batch[0] avg_epoch_loss=3.351244\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:02 INFO 139692952823616] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=3.35124444962\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:02 INFO 139692952823616] Epoch[53] Batch[5] avg_epoch_loss=3.196942\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:02 INFO 139692952823616] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=3.19694232941\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:02 INFO 139692952823616] Epoch[53] Batch [5]#011Speed: 564.36 samples/sec#011loss=3.196942\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:03 INFO 139692952823616] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1639.8391723632812, \"sum\": 1639.8391723632812, \"min\": 1639.8391723632812}}, \"EndTime\": 1612633863.398194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633861.757851}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:03 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=386.593827615 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:03 INFO 139692952823616] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:03 INFO 139692952823616] #quality_metric: host=algo-1, epoch=53, train loss <loss>=3.29564507008\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:03 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:03 INFO 139692952823616] Epoch[54] Batch[0] avg_epoch_loss=3.073045\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:03 INFO 139692952823616] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=3.0730445385\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:04 INFO 139692952823616] Epoch[54] Batch[5] avg_epoch_loss=3.328455\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:04 INFO 139692952823616] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=3.32845540841\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:04 INFO 139692952823616] Epoch[54] Batch [5]#011Speed: 530.95 samples/sec#011loss=3.328455\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] Epoch[54] Batch[10] avg_epoch_loss=3.297995\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=3.26144218445\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] Epoch[54] Batch [10]#011Speed: 479.03 samples/sec#011loss=3.261442\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1862.5259399414062, \"sum\": 1862.5259399414062, \"min\": 1862.5259399414062}}, \"EndTime\": 1612633865.261281, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633863.398279}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=360.777299043 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] #quality_metric: host=algo-1, epoch=54, train loss <loss>=3.29799485207\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] Epoch[55] Batch[0] avg_epoch_loss=3.363108\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:05 INFO 139692952823616] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=3.36310791969\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:06 INFO 139692952823616] Epoch[55] Batch[5] avg_epoch_loss=3.167772\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:06 INFO 139692952823616] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=3.16777193546\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:06 INFO 139692952823616] Epoch[55] Batch [5]#011Speed: 748.72 samples/sec#011loss=3.167772\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:06 INFO 139692952823616] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1384.8321437835693, \"sum\": 1384.8321437835693, \"min\": 1384.8321437835693}}, \"EndTime\": 1612633866.646685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633865.261364}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:06 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=461.387491018 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:06 INFO 139692952823616] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:06 INFO 139692952823616] #quality_metric: host=algo-1, epoch=55, train loss <loss>=3.16353940964\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:06 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:07 INFO 139692952823616] Epoch[56] Batch[0] avg_epoch_loss=3.094502\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:07 INFO 139692952823616] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=3.09450244904\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:07 INFO 139692952823616] Epoch[56] Batch[5] avg_epoch_loss=3.195549\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:07 INFO 139692952823616] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=3.19554917018\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:07 INFO 139692952823616] Epoch[56] Batch [5]#011Speed: 726.96 samples/sec#011loss=3.195549\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] Epoch[56] Batch[10] avg_epoch_loss=3.104309\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=2.99482049942\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] Epoch[56] Batch [10]#011Speed: 638.64 samples/sec#011loss=2.994820\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1506.335973739624, \"sum\": 1506.335973739624, \"min\": 1506.335973739624}}, \"EndTime\": 1612633868.15358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633866.646768}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=432.805346448 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] #quality_metric: host=algo-1, epoch=56, train loss <loss>=3.10430886529\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] Epoch[57] Batch[0] avg_epoch_loss=3.422852\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:08 INFO 139692952823616] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=3.42285203934\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:09 INFO 139692952823616] Epoch[57] Batch[5] avg_epoch_loss=3.294292\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:09 INFO 139692952823616] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=3.29429248969\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:09 INFO 139692952823616] Epoch[57] Batch [5]#011Speed: 748.46 samples/sec#011loss=3.294292\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:09 INFO 139692952823616] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1374.0010261535645, \"sum\": 1374.0010261535645, \"min\": 1374.0010261535645}}, \"EndTime\": 1612633869.528119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633868.153658}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:09 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=444.657933206 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:09 INFO 139692952823616] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:09 INFO 139692952823616] #quality_metric: host=algo-1, epoch=57, train loss <loss>=3.33025126457\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:09 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] Epoch[58] Batch[0] avg_epoch_loss=2.999302\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.99930238724\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] Epoch[58] Batch[5] avg_epoch_loss=3.096023\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=3.09602288405\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] Epoch[58] Batch [5]#011Speed: 724.50 samples/sec#011loss=3.096023\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1374.9959468841553, \"sum\": 1374.9959468841553, \"min\": 1374.9959468841553}}, \"EndTime\": 1612633870.9036, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633869.528178}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=459.606956661 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] #quality_metric: host=algo-1, epoch=58, train loss <loss>=3.16240587234\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:10 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:11 INFO 139692952823616] Epoch[59] Batch[0] avg_epoch_loss=3.245795\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:11 INFO 139692952823616] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=3.24579453468\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:11 INFO 139692952823616] Epoch[59] Batch[5] avg_epoch_loss=3.126705\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:11 INFO 139692952823616] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=3.12670469284\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:11 INFO 139692952823616] Epoch[59] Batch [5]#011Speed: 736.01 samples/sec#011loss=3.126705\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] Epoch[59] Batch[10] avg_epoch_loss=3.349770\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=3.61744894981\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] Epoch[59] Batch [10]#011Speed: 628.55 samples/sec#011loss=3.617449\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1486.5570068359375, \"sum\": 1486.5570068359375, \"min\": 1486.5570068359375}}, \"EndTime\": 1612633872.390718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633870.903661}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=444.614480772 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] #quality_metric: host=algo-1, epoch=59, train loss <loss>=3.34977026419\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] Epoch[60] Batch[0] avg_epoch_loss=3.277457\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:12 INFO 139692952823616] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=3.27745652199\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] Epoch[60] Batch[5] avg_epoch_loss=3.181771\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=3.18177076181\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] Epoch[60] Batch [5]#011Speed: 721.70 samples/sec#011loss=3.181771\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] Epoch[60] Batch[10] avg_epoch_loss=3.300740\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=3.44350223541\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] Epoch[60] Batch [10]#011Speed: 667.71 samples/sec#011loss=3.443502\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1472.3658561706543, \"sum\": 1472.3658561706543, \"min\": 1472.3658561706543}}, \"EndTime\": 1612633873.86364, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633872.390801}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=458.410950742 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] #quality_metric: host=algo-1, epoch=60, train loss <loss>=3.30073961345\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:13 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:14 INFO 139692952823616] Epoch[61] Batch[0] avg_epoch_loss=2.933932\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:14 INFO 139692952823616] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.93393230438\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:14 INFO 139692952823616] Epoch[61] Batch[5] avg_epoch_loss=3.141777\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:14 INFO 139692952823616] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=3.14177719752\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:14 INFO 139692952823616] Epoch[61] Batch [5]#011Speed: 735.88 samples/sec#011loss=3.141777\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] Epoch[61] Batch[10] avg_epoch_loss=3.225095\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=3.32507719994\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] Epoch[61] Batch [10]#011Speed: 669.71 samples/sec#011loss=3.325077\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1458.7948322296143, \"sum\": 1458.7948322296143, \"min\": 1458.7948322296143}}, \"EndTime\": 1612633875.323045, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633873.863714}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=444.850486555 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] #quality_metric: host=algo-1, epoch=61, train loss <loss>=3.22509538044\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] Epoch[62] Batch[0] avg_epoch_loss=3.405327\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:15 INFO 139692952823616] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=3.4053273201\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] Epoch[62] Batch[5] avg_epoch_loss=3.204132\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=3.20413192113\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] Epoch[62] Batch [5]#011Speed: 749.68 samples/sec#011loss=3.204132\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] Epoch[62] Batch[10] avg_epoch_loss=3.237939\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=3.27850737572\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] Epoch[62] Batch [10]#011Speed: 651.22 samples/sec#011loss=3.278507\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1460.2360725402832, \"sum\": 1460.2360725402832, \"min\": 1460.2360725402832}}, \"EndTime\": 1612633876.783893, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633875.323126}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=450.575654647 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] #quality_metric: host=algo-1, epoch=62, train loss <loss>=3.23793894594\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:16 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:17 INFO 139692952823616] Epoch[63] Batch[0] avg_epoch_loss=2.950918\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:17 INFO 139692952823616] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.95091819763\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:17 INFO 139692952823616] Epoch[63] Batch[5] avg_epoch_loss=3.160373\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:17 INFO 139692952823616] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=3.16037344933\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:17 INFO 139692952823616] Epoch[63] Batch [5]#011Speed: 740.77 samples/sec#011loss=3.160373\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] Epoch[63] Batch[10] avg_epoch_loss=3.216348\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=3.28351683617\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] Epoch[63] Batch [10]#011Speed: 616.73 samples/sec#011loss=3.283517\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1490.358829498291, \"sum\": 1490.358829498291, \"min\": 1490.358829498291}}, \"EndTime\": 1612633878.274817, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633876.783971}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=441.470866559 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] #quality_metric: host=algo-1, epoch=63, train loss <loss>=3.21634771607\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] Epoch[64] Batch[0] avg_epoch_loss=3.262200\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:18 INFO 139692952823616] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=3.26220035553\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:19 INFO 139692952823616] Epoch[64] Batch[5] avg_epoch_loss=3.161159\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:19 INFO 139692952823616] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=3.16115891933\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:19 INFO 139692952823616] Epoch[64] Batch [5]#011Speed: 758.82 samples/sec#011loss=3.161159\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:19 INFO 139692952823616] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1375.304937362671, \"sum\": 1375.304937362671, \"min\": 1375.304937362671}}, \"EndTime\": 1612633879.65069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633878.274893}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:19 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=465.319883366 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:19 INFO 139692952823616] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:19 INFO 139692952823616] #quality_metric: host=algo-1, epoch=64, train loss <loss>=3.19942450523\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:19 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:20 INFO 139692952823616] Epoch[65] Batch[0] avg_epoch_loss=3.054713\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:20 INFO 139692952823616] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=3.05471277237\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:20 INFO 139692952823616] Epoch[65] Batch[5] avg_epoch_loss=3.141460\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:20 INFO 139692952823616] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=3.1414595445\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:20 INFO 139692952823616] Epoch[65] Batch [5]#011Speed: 753.36 samples/sec#011loss=3.141460\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1352.1409034729004, \"sum\": 1352.1409034729004, \"min\": 1352.1409034729004}}, \"EndTime\": 1612633881.00339, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633879.65075}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=459.974813369 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] #quality_metric: host=algo-1, epoch=65, train loss <loss>=3.22330775261\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] Epoch[66] Batch[0] avg_epoch_loss=3.316609\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=3.31660914421\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] Epoch[66] Batch[5] avg_epoch_loss=3.247533\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=3.24753252665\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:21 INFO 139692952823616] Epoch[66] Batch [5]#011Speed: 736.74 samples/sec#011loss=3.247533\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:22 INFO 139692952823616] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1364.9320602416992, \"sum\": 1364.9320602416992, \"min\": 1364.9320602416992}}, \"EndTime\": 1612633882.368917, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633881.003461}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:22 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=461.528623505 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:22 INFO 139692952823616] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:22 INFO 139692952823616] #quality_metric: host=algo-1, epoch=66, train loss <loss>=3.18946609497\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:22 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:22 INFO 139692952823616] Epoch[67] Batch[0] avg_epoch_loss=3.275663\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:22 INFO 139692952823616] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=3.2756626606\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] Epoch[67] Batch[5] avg_epoch_loss=3.250811\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=3.25081137816\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] Epoch[67] Batch [5]#011Speed: 677.88 samples/sec#011loss=3.250811\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] Epoch[67] Batch[10] avg_epoch_loss=3.165602\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=3.06334981918\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] Epoch[67] Batch [10]#011Speed: 651.04 samples/sec#011loss=3.063350\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1509.929895401001, \"sum\": 1509.929895401001, \"min\": 1509.929895401001}}, \"EndTime\": 1612633883.879396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633882.368979}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=436.411440672 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] #quality_metric: host=algo-1, epoch=67, train loss <loss>=3.16560157863\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:23 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:24 INFO 139692952823616] Epoch[68] Batch[0] avg_epoch_loss=3.065075\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:24 INFO 139692952823616] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=3.06507539749\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:24 INFO 139692952823616] Epoch[68] Batch[5] avg_epoch_loss=3.215001\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:24 INFO 139692952823616] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=3.21500146389\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:24 INFO 139692952823616] Epoch[68] Batch [5]#011Speed: 740.94 samples/sec#011loss=3.215001\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:25 INFO 139692952823616] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1420.0069904327393, \"sum\": 1420.0069904327393, \"min\": 1420.0069904327393}}, \"EndTime\": 1612633885.299918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633883.879472}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:25 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=438.693406348 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:25 INFO 139692952823616] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:25 INFO 139692952823616] #quality_metric: host=algo-1, epoch=68, train loss <loss>=3.24418568611\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:25 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:25 INFO 139692952823616] Epoch[69] Batch[0] avg_epoch_loss=3.162995\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:25 INFO 139692952823616] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=3.1629948616\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:26 INFO 139692952823616] Epoch[69] Batch[5] avg_epoch_loss=3.203087\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:26 INFO 139692952823616] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=3.20308657487\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:26 INFO 139692952823616] Epoch[69] Batch [5]#011Speed: 734.79 samples/sec#011loss=3.203087\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:26 INFO 139692952823616] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1396.2860107421875, \"sum\": 1396.2860107421875, \"min\": 1396.2860107421875}}, \"EndTime\": 1612633886.696761, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633885.3}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:26 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=446.139550321 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:26 INFO 139692952823616] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:26 INFO 139692952823616] #quality_metric: host=algo-1, epoch=69, train loss <loss>=3.18087308407\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:26 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:27 INFO 139692952823616] Epoch[70] Batch[0] avg_epoch_loss=3.184654\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:27 INFO 139692952823616] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=3.18465352058\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:27 INFO 139692952823616] Epoch[70] Batch[5] avg_epoch_loss=3.157027\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:27 INFO 139692952823616] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=3.15702744325\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:27 INFO 139692952823616] Epoch[70] Batch [5]#011Speed: 761.93 samples/sec#011loss=3.157027\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:28 INFO 139692952823616] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1393.286943435669, \"sum\": 1393.286943435669, \"min\": 1393.286943435669}}, \"EndTime\": 1612633888.090643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633886.69686}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:28 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=442.80020341 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:28 INFO 139692952823616] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:28 INFO 139692952823616] #quality_metric: host=algo-1, epoch=70, train loss <loss>=3.22516252995\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:28 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:28 INFO 139692952823616] Epoch[71] Batch[0] avg_epoch_loss=3.257097\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:28 INFO 139692952823616] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=3.25709652901\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] Epoch[71] Batch[5] avg_epoch_loss=3.059586\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=3.05958644549\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] Epoch[71] Batch [5]#011Speed: 726.44 samples/sec#011loss=3.059586\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] Epoch[71] Batch[10] avg_epoch_loss=3.129067\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=3.21244373322\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] Epoch[71] Batch [10]#011Speed: 666.33 samples/sec#011loss=3.212444\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1490.6139373779297, \"sum\": 1490.6139373779297, \"min\": 1490.6139373779297}}, \"EndTime\": 1612633889.581814, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633888.090724}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=440.724574662 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] #quality_metric: host=algo-1, epoch=71, train loss <loss>=3.12906703082\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:29 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] Epoch[72] Batch[0] avg_epoch_loss=3.028953\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=3.02895259857\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] Epoch[72] Batch[5] avg_epoch_loss=3.098861\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=3.09886054198\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] Epoch[72] Batch [5]#011Speed: 732.81 samples/sec#011loss=3.098861\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1389.953851699829, \"sum\": 1389.953851699829, \"min\": 1389.953851699829}}, \"EndTime\": 1612633890.972323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633889.581891}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=457.529201952 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] #quality_metric: host=algo-1, epoch=72, train loss <loss>=3.08634979725\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:30 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:31 INFO 139692952823616] Epoch[73] Batch[0] avg_epoch_loss=2.992019\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:31 INFO 139692952823616] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.99201917648\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:31 INFO 139692952823616] Epoch[73] Batch[5] avg_epoch_loss=3.026780\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:31 INFO 139692952823616] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=3.02678012848\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:31 INFO 139692952823616] Epoch[73] Batch [5]#011Speed: 764.32 samples/sec#011loss=3.026780\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:32 INFO 139692952823616] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1368.330955505371, \"sum\": 1368.330955505371, \"min\": 1368.330955505371}}, \"EndTime\": 1612633892.341211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633890.972405}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:32 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=455.990299138 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:32 INFO 139692952823616] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:32 INFO 139692952823616] #quality_metric: host=algo-1, epoch=73, train loss <loss>=3.15468127728\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:32 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:32 INFO 139692952823616] Epoch[74] Batch[0] avg_epoch_loss=3.383172\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:32 INFO 139692952823616] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=3.38317227364\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] Epoch[74] Batch[5] avg_epoch_loss=3.215893\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=3.21589330832\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] Epoch[74] Batch [5]#011Speed: 721.45 samples/sec#011loss=3.215893\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] Epoch[74] Batch[10] avg_epoch_loss=3.264129\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=3.32201290131\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] Epoch[74] Batch [10]#011Speed: 656.54 samples/sec#011loss=3.322013\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1520.279884338379, \"sum\": 1520.279884338379, \"min\": 1520.279884338379}}, \"EndTime\": 1612633893.862045, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633892.341293}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=436.065407375 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] #quality_metric: host=algo-1, epoch=74, train loss <loss>=3.26412948695\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:33 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:34 INFO 139692952823616] Epoch[75] Batch[0] avg_epoch_loss=3.188066\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:34 INFO 139692952823616] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=3.18806600571\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:34 INFO 139692952823616] Epoch[75] Batch[5] avg_epoch_loss=3.140034\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:34 INFO 139692952823616] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=3.14003403982\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:34 INFO 139692952823616] Epoch[75] Batch [5]#011Speed: 761.53 samples/sec#011loss=3.140034\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:35 INFO 139692952823616] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1365.4589653015137, \"sum\": 1365.4589653015137, \"min\": 1365.4589653015137}}, \"EndTime\": 1612633895.22804, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633893.862122}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:35 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=461.351910244 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:35 INFO 139692952823616] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:35 INFO 139692952823616] #quality_metric: host=algo-1, epoch=75, train loss <loss>=3.18650066853\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:35 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:35 INFO 139692952823616] Epoch[76] Batch[0] avg_epoch_loss=3.163412\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:35 INFO 139692952823616] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=3.1634118557\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:36 INFO 139692952823616] Epoch[76] Batch[5] avg_epoch_loss=3.218884\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:36 INFO 139692952823616] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=3.21888418992\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:36 INFO 139692952823616] Epoch[76] Batch [5]#011Speed: 749.26 samples/sec#011loss=3.218884\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:36 INFO 139692952823616] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1354.8200130462646, \"sum\": 1354.8200130462646, \"min\": 1354.8200130462646}}, \"EndTime\": 1612633896.583406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633895.228102}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:36 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=471.612848711 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:36 INFO 139692952823616] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:36 INFO 139692952823616] #quality_metric: host=algo-1, epoch=76, train loss <loss>=3.1899343729\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:36 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:37 INFO 139692952823616] Epoch[77] Batch[0] avg_epoch_loss=2.920583\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:37 INFO 139692952823616] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.92058300972\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:37 INFO 139692952823616] Epoch[77] Batch[5] avg_epoch_loss=2.920850\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:37 INFO 139692952823616] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.92084991932\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:37 INFO 139692952823616] Epoch[77] Batch [5]#011Speed: 748.81 samples/sec#011loss=2.920850\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] Epoch[77] Batch[10] avg_epoch_loss=3.007880\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=3.11231622696\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] Epoch[77] Batch [10]#011Speed: 653.65 samples/sec#011loss=3.112316\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] processed a total of 708 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1557.3630332946777, \"sum\": 1557.3630332946777, \"min\": 1557.3630332946777}}, \"EndTime\": 1612633898.141327, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633896.583471}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=454.582535517 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] #quality_metric: host=algo-1, epoch=77, train loss <loss>=3.14399292072\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] Epoch[78] Batch[0] avg_epoch_loss=3.203632\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:38 INFO 139692952823616] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=3.20363163948\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:39 INFO 139692952823616] Epoch[78] Batch[5] avg_epoch_loss=3.159109\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:39 INFO 139692952823616] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=3.15910919507\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:39 INFO 139692952823616] Epoch[78] Batch [5]#011Speed: 765.82 samples/sec#011loss=3.159109\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:39 INFO 139692952823616] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1353.4529209136963, \"sum\": 1353.4529209136963, \"min\": 1353.4529209136963}}, \"EndTime\": 1612633899.495362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633898.141398}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:39 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=472.088428853 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:39 INFO 139692952823616] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:39 INFO 139692952823616] #quality_metric: host=algo-1, epoch=78, train loss <loss>=3.16991159916\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:39 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] Epoch[79] Batch[0] avg_epoch_loss=2.891664\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.89166355133\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] Epoch[79] Batch[5] avg_epoch_loss=3.036115\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=3.03611465295\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] Epoch[79] Batch [5]#011Speed: 686.07 samples/sec#011loss=3.036115\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1387.821912765503, \"sum\": 1387.821912765503, \"min\": 1387.821912765503}}, \"EndTime\": 1612633900.883756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633899.495434}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=453.194857916 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] #quality_metric: host=algo-1, epoch=79, train loss <loss>=3.0723785162\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:40 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:41 INFO 139692952823616] Epoch[80] Batch[0] avg_epoch_loss=3.363694\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:41 INFO 139692952823616] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=3.36369371414\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:41 INFO 139692952823616] Epoch[80] Batch[5] avg_epoch_loss=3.207516\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:41 INFO 139692952823616] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=3.20751555761\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:41 INFO 139692952823616] Epoch[80] Batch [5]#011Speed: 766.31 samples/sec#011loss=3.207516\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Epoch[80] Batch[10] avg_epoch_loss=3.162108\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=3.10761795044\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Epoch[80] Batch [10]#011Speed: 668.82 samples/sec#011loss=3.107618\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1456.9199085235596, \"sum\": 1456.9199085235596, \"min\": 1456.9199085235596}}, \"EndTime\": 1612633902.341217, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633900.88382}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] #throughput_metric: host=algo-1, train throughput=447.486572717 records/second\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] #quality_metric: host=algo-1, epoch=80, train loss <loss>=3.16210755435\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Loading parameters from best epoch (40)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 10.998010635375977, \"sum\": 10.998010635375977, \"min\": 10.998010635375977}}, \"EndTime\": 1612633902.352842, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633902.341285}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] stopping training now\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Final loss: 3.0249562426 (occurred at epoch 40)\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] #quality_metric: host=algo-1, train final_loss <loss>=3.0249562426\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 WARNING 139692952823616] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 295.2001094818115, \"sum\": 295.2001094818115, \"min\": 295.2001094818115}}, \"EndTime\": 1612633902.648862, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633902.352893}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 392.86088943481445, \"sum\": 392.86088943481445, \"min\": 392.86088943481445}}, \"EndTime\": 1612633902.746488, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633902.648942}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 15.324831008911133, \"sum\": 15.324831008911133, \"min\": 15.324831008911133}}, \"EndTime\": 1612633902.761941, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633902.746568}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:42 INFO 139692952823616] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.028848648071289062, \"sum\": 0.028848648071289062, \"min\": 0.028848648071289062}}, \"EndTime\": 1612633902.76271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633902.761995}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 3119.3268299102783, \"sum\": 3119.3268299102783, \"min\": 3119.3268299102783}}, \"EndTime\": 1612633905.882004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633902.762761}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, RMSE): 48.7859330183\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, mean_absolute_QuantileLoss): 47601.594559633726\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, mean_wQuantileLoss): 0.09087978769900941\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.1]): 0.07639031288033689\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.2]): 0.0952764078611399\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.3]): 0.10501530336259805\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.4]): 0.10869155169553449\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.5]): 0.10798039915928957\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.6]): 0.1025786696676747\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.7]): 0.09272286724914165\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.8]): 0.07714562401666708\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #test_score (algo-1, wQuantileLoss[0.9]): 0.0521169533987024\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.090879787699\u001b[0m\n",
      "\u001b[34m[02/06/2021 17:51:45 INFO 139692952823616] #quality_metric: host=algo-1, test RMSE <loss>=48.7859330183\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 121878.6518573761, \"sum\": 121878.6518573761, \"min\": 121878.6518573761}, \"setuptime\": {\"count\": 1, \"max\": 9.457826614379883, \"sum\": 9.457826614379883, \"min\": 9.457826614379883}}, \"EndTime\": 1612633905.908959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612633905.882071}\n",
      "\u001b[0m\n",
      "\n",
      "2021-02-06 17:52:08 Uploading - Uploading generated training model\n",
      "2021-02-06 17:52:08 Completed - Training job completed\n",
      "ProfilerReport-1612633535: NoIssuesFound\n",
      "Training seconds: 230\n",
      "Billable seconds: 230\n",
      "CPU times: user 1.12 s, sys: 53.2 ms, total: 1.18 s\n",
      "Wall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "congressional-desktop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 271 ms, sys: 10.4 ms, total: 281 ms\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "otherwise-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous step creates endpoint. \n",
    "# Now I will use it to predict future values in timeseries.\n",
    "# I will use one list from timeseries object created at start of this notebook.\n",
    "response = predictor.predict(ts=timeseries[10], quantiles=[0.20, 0.5, 0.80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "confident-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "coastal-assets",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00</th>\n",
       "      <td>33.574574</td>\n",
       "      <td>29.195810</td>\n",
       "      <td>31.209879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00</th>\n",
       "      <td>32.143036</td>\n",
       "      <td>28.162317</td>\n",
       "      <td>29.888121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 06:00:00</th>\n",
       "      <td>35.214035</td>\n",
       "      <td>29.529940</td>\n",
       "      <td>32.091263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 08:00:00</th>\n",
       "      <td>34.765564</td>\n",
       "      <td>29.588226</td>\n",
       "      <td>32.021854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 10:00:00</th>\n",
       "      <td>49.048203</td>\n",
       "      <td>42.026443</td>\n",
       "      <td>45.086349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 12:00:00</th>\n",
       "      <td>43.697144</td>\n",
       "      <td>35.908176</td>\n",
       "      <td>40.080215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 14:00:00</th>\n",
       "      <td>41.989399</td>\n",
       "      <td>36.100761</td>\n",
       "      <td>38.465931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 16:00:00</th>\n",
       "      <td>48.982998</td>\n",
       "      <td>40.888206</td>\n",
       "      <td>44.579288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 18:00:00</th>\n",
       "      <td>77.754776</td>\n",
       "      <td>67.165169</td>\n",
       "      <td>73.125053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 20:00:00</th>\n",
       "      <td>80.828178</td>\n",
       "      <td>70.326302</td>\n",
       "      <td>75.470016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 22:00:00</th>\n",
       "      <td>65.559944</td>\n",
       "      <td>57.836010</td>\n",
       "      <td>60.907204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 00:00:00</th>\n",
       "      <td>41.929585</td>\n",
       "      <td>36.465294</td>\n",
       "      <td>39.842648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 02:00:00</th>\n",
       "      <td>36.838985</td>\n",
       "      <td>31.254065</td>\n",
       "      <td>33.844673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 04:00:00</th>\n",
       "      <td>35.076324</td>\n",
       "      <td>30.091309</td>\n",
       "      <td>32.139492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 06:00:00</th>\n",
       "      <td>36.677147</td>\n",
       "      <td>31.707785</td>\n",
       "      <td>33.879761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 08:00:00</th>\n",
       "      <td>38.605076</td>\n",
       "      <td>32.996685</td>\n",
       "      <td>35.397987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 10:00:00</th>\n",
       "      <td>50.461540</td>\n",
       "      <td>42.213509</td>\n",
       "      <td>45.837109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 12:00:00</th>\n",
       "      <td>45.742542</td>\n",
       "      <td>37.608063</td>\n",
       "      <td>41.659092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 14:00:00</th>\n",
       "      <td>45.163490</td>\n",
       "      <td>39.056446</td>\n",
       "      <td>42.067135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 16:00:00</th>\n",
       "      <td>52.242702</td>\n",
       "      <td>43.108707</td>\n",
       "      <td>46.812256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 18:00:00</th>\n",
       "      <td>78.866844</td>\n",
       "      <td>66.444504</td>\n",
       "      <td>71.626236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 20:00:00</th>\n",
       "      <td>79.479019</td>\n",
       "      <td>69.269875</td>\n",
       "      <td>74.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 22:00:00</th>\n",
       "      <td>63.959385</td>\n",
       "      <td>56.128380</td>\n",
       "      <td>60.279926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 00:00:00</th>\n",
       "      <td>41.588837</td>\n",
       "      <td>35.885349</td>\n",
       "      <td>38.727486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 02:00:00</th>\n",
       "      <td>35.600231</td>\n",
       "      <td>31.013960</td>\n",
       "      <td>33.144390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 04:00:00</th>\n",
       "      <td>33.541985</td>\n",
       "      <td>29.101831</td>\n",
       "      <td>31.579119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 06:00:00</th>\n",
       "      <td>32.670235</td>\n",
       "      <td>28.063929</td>\n",
       "      <td>30.741528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 08:00:00</th>\n",
       "      <td>34.464058</td>\n",
       "      <td>28.566622</td>\n",
       "      <td>31.486128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 10:00:00</th>\n",
       "      <td>46.685894</td>\n",
       "      <td>39.938873</td>\n",
       "      <td>43.029530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 12:00:00</th>\n",
       "      <td>43.977070</td>\n",
       "      <td>37.713383</td>\n",
       "      <td>41.266277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 14:00:00</th>\n",
       "      <td>43.756516</td>\n",
       "      <td>36.633667</td>\n",
       "      <td>40.471588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 16:00:00</th>\n",
       "      <td>50.146622</td>\n",
       "      <td>41.840538</td>\n",
       "      <td>45.596462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 18:00:00</th>\n",
       "      <td>76.680939</td>\n",
       "      <td>64.847664</td>\n",
       "      <td>69.709213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 20:00:00</th>\n",
       "      <td>75.389725</td>\n",
       "      <td>66.165131</td>\n",
       "      <td>70.823738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 22:00:00</th>\n",
       "      <td>61.661522</td>\n",
       "      <td>54.859249</td>\n",
       "      <td>58.577965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04 00:00:00</th>\n",
       "      <td>38.674969</td>\n",
       "      <td>33.264809</td>\n",
       "      <td>35.962269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0.8        0.2        0.5\n",
       "2015-01-01 02:00:00  33.574574  29.195810  31.209879\n",
       "2015-01-01 04:00:00  32.143036  28.162317  29.888121\n",
       "2015-01-01 06:00:00  35.214035  29.529940  32.091263\n",
       "2015-01-01 08:00:00  34.765564  29.588226  32.021854\n",
       "2015-01-01 10:00:00  49.048203  42.026443  45.086349\n",
       "2015-01-01 12:00:00  43.697144  35.908176  40.080215\n",
       "2015-01-01 14:00:00  41.989399  36.100761  38.465931\n",
       "2015-01-01 16:00:00  48.982998  40.888206  44.579288\n",
       "2015-01-01 18:00:00  77.754776  67.165169  73.125053\n",
       "2015-01-01 20:00:00  80.828178  70.326302  75.470016\n",
       "2015-01-01 22:00:00  65.559944  57.836010  60.907204\n",
       "2015-01-02 00:00:00  41.929585  36.465294  39.842648\n",
       "2015-01-02 02:00:00  36.838985  31.254065  33.844673\n",
       "2015-01-02 04:00:00  35.076324  30.091309  32.139492\n",
       "2015-01-02 06:00:00  36.677147  31.707785  33.879761\n",
       "2015-01-02 08:00:00  38.605076  32.996685  35.397987\n",
       "2015-01-02 10:00:00  50.461540  42.213509  45.837109\n",
       "2015-01-02 12:00:00  45.742542  37.608063  41.659092\n",
       "2015-01-02 14:00:00  45.163490  39.056446  42.067135\n",
       "2015-01-02 16:00:00  52.242702  43.108707  46.812256\n",
       "2015-01-02 18:00:00  78.866844  66.444504  71.626236\n",
       "2015-01-02 20:00:00  79.479019  69.269875  74.442200\n",
       "2015-01-02 22:00:00  63.959385  56.128380  60.279926\n",
       "2015-01-03 00:00:00  41.588837  35.885349  38.727486\n",
       "2015-01-03 02:00:00  35.600231  31.013960  33.144390\n",
       "2015-01-03 04:00:00  33.541985  29.101831  31.579119\n",
       "2015-01-03 06:00:00  32.670235  28.063929  30.741528\n",
       "2015-01-03 08:00:00  34.464058  28.566622  31.486128\n",
       "2015-01-03 10:00:00  46.685894  39.938873  43.029530\n",
       "2015-01-03 12:00:00  43.977070  37.713383  41.266277\n",
       "2015-01-03 14:00:00  43.756516  36.633667  40.471588\n",
       "2015-01-03 16:00:00  50.146622  41.840538  45.596462\n",
       "2015-01-03 18:00:00  76.680939  64.847664  69.709213\n",
       "2015-01-03 20:00:00  75.389725  66.165131  70.823738\n",
       "2015-01-03 22:00:00  61.661522  54.859249  58.577965\n",
       "2015-01-04 00:00:00  38.674969  33.264809  35.962269"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "atlantic-september",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts10 = timeseries[10][-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "athletic-sessions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "predict10 = response['0.5']\n",
    "plt.plot(ts10)\n",
    "plt.plot(predict10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "crude-graham",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-01 02:00:00    31.209879\n",
       "2015-01-01 04:00:00    29.888121\n",
       "2015-01-01 06:00:00    32.091263\n",
       "2015-01-01 08:00:00    32.021854\n",
       "2015-01-01 10:00:00    45.086349\n",
       "2015-01-01 12:00:00    40.080215\n",
       "2015-01-01 14:00:00    38.465931\n",
       "2015-01-01 16:00:00    44.579288\n",
       "2015-01-01 18:00:00    73.125053\n",
       "2015-01-01 20:00:00    75.470016\n",
       "2015-01-01 22:00:00    60.907204\n",
       "2015-01-02 00:00:00    39.842648\n",
       "2015-01-02 02:00:00    33.844673\n",
       "2015-01-02 04:00:00    32.139492\n",
       "2015-01-02 06:00:00    33.879761\n",
       "2015-01-02 08:00:00    35.397987\n",
       "2015-01-02 10:00:00    45.837109\n",
       "2015-01-02 12:00:00    41.659092\n",
       "2015-01-02 14:00:00    42.067135\n",
       "2015-01-02 16:00:00    46.812256\n",
       "2015-01-02 18:00:00    71.626236\n",
       "2015-01-02 20:00:00    74.442200\n",
       "2015-01-02 22:00:00    60.279926\n",
       "2015-01-03 00:00:00    38.727486\n",
       "2015-01-03 02:00:00    33.144390\n",
       "2015-01-03 04:00:00    31.579119\n",
       "2015-01-03 06:00:00    30.741528\n",
       "2015-01-03 08:00:00    31.486128\n",
       "2015-01-03 10:00:00    43.029530\n",
       "2015-01-03 12:00:00    41.266277\n",
       "2015-01-03 14:00:00    40.471588\n",
       "2015-01-03 16:00:00    45.596462\n",
       "2015-01-03 18:00:00    69.709213\n",
       "2015-01-03 20:00:00    70.823738\n",
       "2015-01-03 22:00:00    58.577965\n",
       "2015-01-04 00:00:00    35.962269\n",
       "Freq: 2H, Name: 0.5, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict10 = response['0.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "varied-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_write_path = 's3://sagemaker-us-east-1-686433372380/sagemaker/electricity/response10.csv'\n",
    "#df.to_csv('s3://experimental/playground/temp_csv/dummy.csv', index=False)\n",
    "response.to_csv(csv_write_path, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******Remember to delete endpoint at end of the session before you close this book and leave.*******"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
